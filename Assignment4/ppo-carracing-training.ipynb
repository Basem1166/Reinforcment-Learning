{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8193a14f","cell_type":"markdown","source":"## 1. Install Dependencies","metadata":{}},{"id":"f5d522c0","cell_type":"code","source":"import subprocess\nimport sys\n\n# Upgrade gymnasium to latest version\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gymnasium\", \"--upgrade\"])\n!pip install swig\n!pip install \"gymnasium[box2d]\"\nimport gymnasium as gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport os\nfrom collections import deque\nfrom datetime import datetime\nimport csv\nfrom typing import Tuple, Any\nfrom torch.distributions import Normal\nimport matplotlib.pyplot as plt\nfrom gymnasium.wrappers import FrameStackObservation, GrayscaleObservation\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:12:52.800612Z","iopub.execute_input":"2025-12-10T23:12:52.800937Z","iopub.status.idle":"2025-12-10T23:13:02.731457Z","shell.execute_reply.started":"2025-12-10T23:12:52.800896Z","shell.execute_reply":"2025-12-10T23:13:02.730484Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\nRequirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\nRequirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\nRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\nRequirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[box2d]) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[box2d]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[box2d]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[box2d]) (2024.2.0)\nUsing device: cuda\n","output_type":"stream"}],"execution_count":73},{"id":"eae62b00","cell_type":"markdown","source":"## 3. Image Preprocessing Wrappers","metadata":{}},{"id":"46a294d1","cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport cv2\nfrom collections import deque\n\n\n# ---- Action Repeat (Frame Skip) ----\nclass ActionRepeatWrapper(gym.Wrapper):\n    \"\"\"Repeat the chosen action for `repeat` frames and sum rewards.\"\"\"\n    \n    def __init__(self, env, repeat=4):\n        super().__init__(env)\n        self.repeat = repeat\n\n    def step(self, action):\n        total_reward = 0.0\n        terminated = False\n        truncated = False\n        info = {}\n\n        for _ in range(self.repeat):\n            obs, reward, terminated, truncated, info = self.env.step(action)\n            total_reward += reward\n            if terminated or truncated:\n                break\n\n        return obs, total_reward, terminated, truncated, info\n\n\n# ---- Grayscale + Resize ----\nclass GrayscaleResizeWrapper(gym.ObservationWrapper):\n    \"\"\"Convert RGB to grayscale and resize to 84x84. Output: (1, H, W).\"\"\"\n    \n    def __init__(self, env, shape=(84, 84)):\n        super().__init__(env)\n        self.shape = shape\n        \n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(1, shape[0], shape[1]),\n            dtype=np.uint8\n        )\n    \n    def observation(self, obs):\n        # Convert to grayscale\n        gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n        # Resize to 84×84\n        resized = cv2.resize(gray, self.shape, interpolation=cv2.INTER_AREA)\n        # Add channel dimension (C, H, W)\n        return resized[np.newaxis, :, :]\n\n\n# ---- Frame Stacking ----\nclass FrameStackingWrapper(gym.ObservationWrapper):\n    \"\"\"Stack last N grayscale frames. Output: (N, H, W).\"\"\"\n    \n    def __init__(self, env, num_frames: int = 4):\n        super().__init__(env)\n        self.num_frames = num_frames\n\n        c, h, w = env.observation_space.shape  # should be (1, 84, 84)\n\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(num_frames, h, w),\n            dtype=np.uint8\n        )\n\n        self.frames = deque(maxlen=num_frames)\n    \n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        self.frames.clear()\n        for _ in range(self.num_frames):\n            self.frames.append(obs)  # obs is (1,84,84)\n        return self._get_stacked(), info\n    \n    def step(self, action):\n        obs, reward, terminated, truncated, info = self.env.step(action)\n        self.frames.append(obs)\n        return self._get_stacked(), reward, terminated, truncated, info\n    \n    def _get_stacked(self):\n        # Concatenate frames along channel dimension -> (num_frames, 84, 84)\n        return np.concatenate(list(self.frames), axis=0)\n\n\n# ---- Full preprocessing pipeline ----\nclass CarRacingPreprocessor:\n    \"\"\"Apply preprocessing: ActionRepeat → Grayscale+Resize → FrameStack\"\"\"\n    \n    @staticmethod\n    def apply(env: gym.Env, use_grayscale=True, num_frames=4, action_repeat=4) -> gym.Env:\n        \n        if action_repeat > 1:\n            env = ActionRepeatWrapper(env, repeat=action_repeat)\n\n        if use_grayscale:\n            env = GrayscaleResizeWrapper(env, shape=(84, 84))\n\n        if num_frames > 1:\n            env = FrameStackingWrapper(env, num_frames=num_frames)\n\n        return env\n\n\nprint(\"Preprocessing wrappers with action repeat, resize, grayscale, and frame stack initialized.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:55:28.281359Z","iopub.execute_input":"2025-12-10T23:55:28.281698Z","iopub.status.idle":"2025-12-10T23:55:28.295168Z","shell.execute_reply.started":"2025-12-10T23:55:28.281668Z","shell.execute_reply":"2025-12-10T23:55:28.294356Z"}},"outputs":[{"name":"stdout","text":"Preprocessing wrappers with action repeat, resize, grayscale, and frame stack initialized.\n","output_type":"stream"}],"execution_count":82},{"id":"53189b8b","cell_type":"markdown","source":"## 4. Neural Network Models","metadata":{}},{"id":"ab0253f8","cell_type":"code","source":"def weights_init_(m):\n    \"\"\"Initialize network weights\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n        torch.nn.init.constant_(m.bias, 0)\n\n\nclass CarRacingCNNPPOEncoder(nn.Module):\n    \"\"\"CNN encoder for CarRacing with grayscale frame stacking\"\"\"\n    \n    def __init__(self, feature_dim: int = 256, input_channels: int = 4):\n        super().__init__()\n        self.feature_dim = feature_dim\n        \n        self.conv = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # -> 32x23x23\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),              # -> 64x10x10\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),              # -> 64x8x8\n            nn.ReLU(inplace=True),\n        )\n        \n        conv_out_dim = 64 * 8 * 8  # 4096\n        self.fc = nn.Linear(conv_out_dim, feature_dim)\n        \n        self.apply(weights_init_)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() == 3:  # (C, H, W)\n            x = x.unsqueeze(0)\n        \n        x = x.float() / 255.0\n        x = self.conv(x)\n        x = x.reshape(x.size(0), -1)\n        x = F.relu(self.fc(x))\n        \n        return x\n\n\nclass PPOActorCNN(nn.Module):\n    \"\"\"Actor network for PPO with CNN encoder\"\"\"\n    \n    def __init__(self, encoder: CarRacingCNNPPOEncoder, action_dim: int, hidden_dim: int = 256):\n        super().__init__()\n        self.encoder = encoder\n        \n        self.fc1 = nn.Linear(encoder.feature_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.mean = nn.Linear(hidden_dim, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n        \n        self.apply(weights_init_)\n    \n    def forward(self, state):\n        features = self.encoder(state)\n        x = torch.tanh(self.fc1(features))\n        x = torch.tanh(self.fc2(x))\n        mean = self.mean(x)\n        std = self.log_std.exp().expand_as(mean)\n        return mean, std\n    \n    def get_dist(self, state):\n        mean, std = self.forward(state)\n        return Normal(mean, std)\n\n\nclass PPOCriticCNN(nn.Module):\n    \"\"\"Critic network for PPO with CNN encoder\"\"\"\n    \n    def __init__(self, encoder: CarRacingCNNPPOEncoder, hidden_dim: int = 256):\n        super().__init__()\n        self.encoder = encoder\n        \n        self.fc1 = nn.Linear(encoder.feature_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.value = nn.Linear(hidden_dim, 1)\n        \n        self.apply(weights_init_)\n    \n    def forward(self, state):\n        features = self.encoder(state)\n        x = torch.tanh(self.fc1(features))\n\ndef weights_init_(m):\n    \"\"\"Initialize network weights\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n        torch.nn.init.constant_(m.bias, 0)\n\n\nclass CarRacingCNNPPOEncoder(nn.Module):\n    \"\"\"CNN encoder for CarRacing with grayscale 84x84 frames and frame stacking\"\"\"\n    \n    def __init__(self, feature_dim: int = 256, input_channels: int = 4):\n        super().__init__()\n        self.feature_dim = feature_dim\n        \n        self.conv = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # 32 x 20 x 20\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),              # 64 x 9 x 9\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),              # 64 x 7 x 7\n            nn.ReLU(inplace=True),\n        )\n        \n        conv_out_dim = 64 * 7 * 7  # 3136\n        self.fc = nn.Linear(conv_out_dim, feature_dim)\n        \n        self.apply(weights_init_)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() == 3:  # (C, H, W)\n            x = x.unsqueeze(0)\n        \n        x = x.float() / 255.0  # normalize\n        x = self.conv(x)\n        x = x.reshape(x.size(0), -1)\n        x = F.relu(self.fc(x))\n        \n        return x\n\n\nclass PPOActorCNN(nn.Module):\n    \"\"\"Actor network for PPO with CNN encoder\"\"\"\n    \n    def __init__(self, encoder: CarRacingCNNPPOEncoder, action_dim: int, hidden_dim: int = 256):\n        super().__init__()\n        self.encoder = encoder\n        \n        self.fc1 = nn.Linear(encoder.feature_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.mean = nn.Linear(hidden_dim, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n        \n        self.apply(weights_init_)\n    \n    def forward(self, state):\n        features = self.encoder(state)\n        x = torch.tanh(self.fc1(features))\n        x = torch.tanh(self.fc2(x))\n        mean = self.mean(x)\n        std = self.log_std.exp().expand_as(mean)\n        return mean, std\n    \n    def get_dist(self, state):\n        mean, std = self.forward(state)\n        return Normal(mean, std)\n\n\nclass PPOCriticCNN(nn.Module):\n    \"\"\"Critic network for PPO with CNN encoder\"\"\"\n    \n    def __init__(self, encoder: CarRacingCNNPPOEncoder, hidden_dim: int = 256):\n        super().__init__()\n        self.encoder = encoder\n        \n        self.fc1 = nn.Linear(encoder.feature_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.value = nn.Linear(hidden_dim, 1)\n        \n        self.apply(weights_init_)\n    \n    def forward(self, state):\n        features = self.encoder(state)\n        x = torch.tanh(self.fc1(features))\n        x = torch.tanh(self.fc2(x))\n        value = self.value(x)\n        return value\n\nprint(\"Network models initialized for 84x84 grayscale stacked frames\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:55:46.749162Z","iopub.execute_input":"2025-12-10T23:55:46.749877Z","iopub.status.idle":"2025-12-10T23:55:46.769644Z","shell.execute_reply.started":"2025-12-10T23:55:46.749852Z","shell.execute_reply":"2025-12-10T23:55:46.768967Z"}},"outputs":[{"name":"stdout","text":"Network models initialized for 84x84 grayscale stacked frames\n","output_type":"stream"}],"execution_count":87},{"id":"f15b8250","cell_type":"markdown","source":"## 5. Rollout Buffer","metadata":{}},{"id":"68272873","cell_type":"code","source":"class RolloutBuffer:\n    \"\"\"Buffer for storing trajectories for PPO\"\"\"\n    \n    def __init__(self):\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.dones = []\n        self.log_probs = []\n        self.values = []\n    \n    def add(self, state, action, reward, done, log_prob, value):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.dones.append(done)\n        self.log_probs.append(log_prob)\n        self.values.append(value)\n    \n    def get(self):\n        return (\n            self.states,\n            self.actions,\n            self.rewards,\n            self.dones,\n            self.log_probs,\n            self.values\n        )\n    \n    def clear(self):\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.dones = []\n        self.log_probs = []\n        self.values = []\n\nprint(\"Rollout buffer initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:13:02.781411Z","iopub.execute_input":"2025-12-10T23:13:02.781654Z","iopub.status.idle":"2025-12-10T23:13:02.799576Z","shell.execute_reply.started":"2025-12-10T23:13:02.781636Z","shell.execute_reply":"2025-12-10T23:13:02.798857Z"}},"outputs":[{"name":"stdout","text":"Rollout buffer initialized\n","output_type":"stream"}],"execution_count":76},{"id":"f3922bfc","cell_type":"markdown","source":"## 6. PPO Agent with CNN","metadata":{}},{"id":"1462b972","cell_type":"code","source":"class PPOAgentCNN:\n    \"\"\"PPO Agent with CNN encoder for image-based observations (84x84 grayscale + frame stacking)\"\"\"\n    \n    def __init__(self, action_dim, hyperparameters, device, feature_dim=256, num_frames=4):\n        self.device = device\n        self.action_dim = action_dim\n        self.feature_dim = feature_dim\n        self.input_channels = num_frames  # match frame stack\n        \n        # Hyperparameters\n        self.gamma = hyperparameters.get('gamma', 0.99)\n        self.gae_lambda = hyperparameters.get('gae_lambda', 0.95)\n        self.lr = hyperparameters.get('lr', 2.5e-4)\n        self.clip_epsilon = hyperparameters.get('clip_epsilon', 0.2)\n        self.value_loss_coef = hyperparameters.get('value_loss_coef', 0.5)\n        self.entropy_coef = hyperparameters.get('entropy_coef', 0.01)\n        self.max_grad_norm = hyperparameters.get('max_grad_norm', 0.5)\n        self.ppo_epochs = hyperparameters.get('ppo_epochs', 10)\n        self.mini_batch_size = hyperparameters.get('mini_batch_size', 64)\n        self.hidden_dim = hyperparameters.get('hidden_dim', 256)\n        \n        # Encoder expects input shape: (num_frames, 84, 84)\n        self.encoder = CarRacingCNNPPOEncoder(\n            feature_dim=feature_dim,\n            input_channels=self.input_channels\n        ).to(device)\n        \n        # Actor and Critic\n        self.actor = PPOActorCNN(self.encoder, action_dim, self.hidden_dim).to(device)\n        self.critic = PPOCriticCNN(self.encoder, self.hidden_dim).to(device)\n        \n        # Optimizer\n        self.optimizer = optim.Adam(\n            list(self.actor.parameters()) + list(self.critic.parameters()),\n            lr=self.lr\n        )\n        \n        # Rollout buffer\n        self.buffer = RolloutBuffer()\n    \n    def select_action(self, state, eval_mode=False):\n        # Ensure state has shape (1, C, H, W)\n        if isinstance(state, np.ndarray):\n            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        else:\n            state = state.unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            if eval_mode:\n                mean, _ = self.actor(state)\n                action = mean\n                return action.cpu().numpy()[0]\n            else:\n                dist = self.actor.get_dist(state)\n                action = dist.sample()\n                log_prob = dist.log_prob(action).sum(dim=-1)\n                value = self.critic(state)\n                return action.cpu().numpy()[0], log_prob.cpu().item(), value.cpu().item()\n    \n    def store_transition(self, state, action, reward, done, log_prob, value):\n        self.buffer.add(state, action, reward, done, log_prob, value)\n    \n    def compute_gae(self, rewards, values, dones, next_value):\n        advantages = []\n        gae = 0\n        values = values + [next_value]\n        for t in reversed(range(len(rewards))):\n            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n            advantages.insert(0, gae)\n        return advantages\n    \n    def update(self, next_state):\n        states, actions, rewards, dones, old_log_probs, values = self.buffer.get()\n        \n        # Compute next value\n        with torch.no_grad():\n            if isinstance(next_state, np.ndarray):\n                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n            else:\n                next_state_tensor = next_state.unsqueeze(0).to(self.device)\n            next_value = self.critic(next_state_tensor).cpu().item()\n        \n        advantages = self.compute_gae(rewards, values, dones, next_value)\n        \n        # Convert to tensors\n        states = torch.FloatTensor(np.array(states)).to(self.device)\n        actions = torch.FloatTensor(np.array(actions)).to(self.device)\n        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)\n        advantages = torch.FloatTensor(advantages).to(self.device)\n        \n        # Normalize advantages\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        returns = advantages + torch.FloatTensor(values).to(self.device)\n        \n        dataset_size = states.size(0)\n        total_policy_loss, total_value_loss, total_entropy = 0, 0, 0\n        update_count = 0\n        \n        # PPO update loop\n        for _ in range(self.ppo_epochs):\n            indices = np.random.permutation(dataset_size)\n            for start in range(0, dataset_size, self.mini_batch_size):\n                end = min(start + self.mini_batch_size, dataset_size)\n                if end - start < 1:\n                    continue\n                idx = indices[start:end]\n                mb_states = states[idx]\n                mb_actions = actions[idx]\n                mb_old_log_probs = old_log_probs[idx]\n                mb_advantages = advantages[idx]\n                mb_returns = returns[idx]\n                \n                dist = self.actor.get_dist(mb_states)\n                new_log_probs = dist.log_prob(mb_actions).sum(dim=-1)\n                entropy = dist.entropy().sum(dim=-1).mean()\n                \n                values_pred = self.critic(mb_states).squeeze()\n                \n                ratio = torch.exp(new_log_probs - mb_old_log_probs)\n                surr1 = ratio * mb_advantages\n                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_advantages\n                policy_loss = -torch.min(surr1, surr2).mean()\n                \n                value_loss = F.mse_loss(values_pred, mb_returns)\n                \n                loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy\n                \n                self.optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(\n                    list(self.actor.parameters()) + list(self.critic.parameters()),\n                    self.max_grad_norm\n                )\n                self.optimizer.step()\n                \n                total_policy_loss += policy_loss.item()\n                total_value_loss += value_loss.item()\n                total_entropy += entropy.item()\n                update_count += 1\n        \n        self.buffer.clear()\n        \n        return {\n            'policy_loss': total_policy_loss / update_count if update_count > 0 else 0,\n            'value_loss': total_value_loss / update_count if update_count > 0 else 0,\n            'entropy': total_entropy / update_count if update_count > 0 else 0\n        }\n    \n    def save(self, filepath):\n        os.makedirs(os.path.dirname(filepath) or '.', exist_ok=True)\n        torch.save({\n            'encoder_state_dict': self.encoder.state_dict(),\n            'actor_state_dict': self.actor.state_dict(),\n            'critic_state_dict': self.critic.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }, filepath)\n    \n    def load(self, filepath):\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n        if 'optimizer_state_dict' in checkpoint:\n            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\nprint(\"PPO Agent initialized (84x84 + frame stack)\") \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:56:07.485059Z","iopub.execute_input":"2025-12-10T23:56:07.485802Z","iopub.status.idle":"2025-12-10T23:56:07.505848Z","shell.execute_reply.started":"2025-12-10T23:56:07.485773Z","shell.execute_reply":"2025-12-10T23:56:07.505297Z"}},"outputs":[{"name":"stdout","text":"PPO Agent initialized (84x84 + frame stack)\n","output_type":"stream"}],"execution_count":88},{"id":"b9545b7b","cell_type":"code","source":"# Hyperparameters for PPO on CarRacing-v3\nHYPERPARAMETERS = {\n    'lr': 2.5e-4,                    # Learning rate (lower for image-based tasks)\n    'gamma': 0.99,                 # Discount factor\n    'gae_lambda': 0.95,            # GAE parameter\n    'clip_epsilon': 0.2,           # PPO clipping parameter\n    'entropy_coef': 0.01,         # Entropy coefficient (exploration)\n    'value_loss_coef': 0.5,        # Value loss weight\n    'max_grad_norm': 0.5,          # Gradient clipping\n    'ppo_epochs': 10,              # PPO update epochs\n    'mini_batch_size': 256,         # Mini-batch size\n    'hidden_dim': 512,             # Network hidden dimension\n}\n\nTRAINING_CONFIG = TRAINING_CONFIG = {\n    'total_timesteps': 375_000,   # Agent steps (1.5M env frames / frameskip=4)\n    'eval_frequency': 25_000,     # Evaluate every N agent steps\n    'eval_episodes': 10,           # Number of eval episodes\n    'save_frequency': 25_000,      # Save every N agent steps\n    'rollout_length': 4096,        # PPO rollout buffer length\n}\n\n\nprint(\"Configuration loaded\")\nprint(f\"Hyperparameters: {HYPERPARAMETERS}\")\nprint(f\"Training config: {TRAINING_CONFIG}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:57:05.464392Z","iopub.execute_input":"2025-12-10T23:57:05.465177Z","iopub.status.idle":"2025-12-10T23:57:05.470599Z","shell.execute_reply.started":"2025-12-10T23:57:05.465148Z","shell.execute_reply":"2025-12-10T23:57:05.469958Z"}},"outputs":[{"name":"stdout","text":"Configuration loaded\nHyperparameters: {'lr': 0.00025, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_epsilon': 0.2, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'ppo_epochs': 10, 'mini_batch_size': 256, 'hidden_dim': 512}\nTraining config: {'total_timesteps': 375000, 'eval_frequency': 25000, 'eval_episodes': 10, 'save_frequency': 25000, 'rollout_length': 4096}\n","output_type":"stream"}],"execution_count":89},{"id":"bba6e26d","cell_type":"markdown","source":"## 7. Hyperparameters Configuration","metadata":{}},{"id":"6c7f4e8c","cell_type":"markdown","source":"## 8. Setup Environment and Agent","metadata":{}},{"id":"17849e90","cell_type":"code","source":"# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Number of frames to stack\nNUM_FRAMES = 4\n\n# Create training environment with preprocessing (grayscale, 84x84, frame stack, optional frameskip)\nenv = gym.make('CarRacing-v3', continuous=True, render_mode=None)\nenv = CarRacingPreprocessor.apply(env, use_grayscale=True, num_frames=NUM_FRAMES)\n\n# Create evaluation environment\neval_env = gym.make('CarRacing-v3', continuous=True, render_mode=None)\neval_env = CarRacingPreprocessor.apply(eval_env, use_grayscale=True, num_frames=NUM_FRAMES)\n\n# Get action dimension\naction_dim = env.action_space.shape[0]\n\nprint(f\"\\nEnvironment: CarRacing-v3\")\nprint(f\"Action dimension: {action_dim}\")\nprint(f\"Observation shape: {env.observation_space.shape}\")  # should be (num_frames, 84, 84)\nprint(f\"Action space: {env.action_space}\")\n\n# Initialize PPO agent\nagent = PPOAgentCNN(\n    action_dim=action_dim,\n    hyperparameters=HYPERPARAMETERS,\n    device=device,\n    feature_dim=256,\n    num_frames=NUM_FRAMES  # matches stacked frames\n)\n\nprint(\"\\nAgent initialized successfully!\")\nprint(f\"Actor parameters: {sum(p.numel() for p in agent.actor.parameters())}\")\nprint(f\"Critic parameters: {sum(p.numel() for p in agent.critic.parameters())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:57:14.634739Z","iopub.execute_input":"2025-12-10T23:57:14.635089Z","iopub.status.idle":"2025-12-10T23:57:14.673218Z","shell.execute_reply.started":"2025-12-10T23:57:14.635068Z","shell.execute_reply":"2025-12-10T23:57:14.672589Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nEnvironment: CarRacing-v3\nAction dimension: 3\nObservation shape: (4, 84, 84)\nAction space: Box([-1.  0.  0.], 1.0, (3,), float32)\n\nAgent initialized successfully!\nActor parameters: 1276838\nCritic parameters: 1275809\n","output_type":"stream"}],"execution_count":90},{"id":"a7d350e2","cell_type":"markdown","source":"## 9. Training Loop","metadata":{}},{"id":"cd49137c","cell_type":"code","source":"# Training parameters\ntotal_timesteps = TRAINING_CONFIG['total_timesteps']\neval_frequency = TRAINING_CONFIG['eval_frequency']\neval_episodes = TRAINING_CONFIG['eval_episodes']\nsave_frequency = TRAINING_CONFIG['save_frequency']\nrollout_length = TRAINING_CONFIG['rollout_length']\n\n# Initialize training variables\nstate, _ = env.reset()  # Gym v3 returns (obs, info)\nstate = np.array(state, dtype=np.uint8)  # Ensure shape matches CNN input\nepisode_reward = 0\nepisode_length = 0\nepisodes_trained = 0\nepisode_rewards = []\ntimestep = 0\nbest_eval_reward = -np.inf\n\n# Training metrics\ntraining_metrics = {\n    'timesteps': [],\n    'episode_rewards': [],\n    'eval_rewards': [],\n    'policy_losses': [],\n    'value_losses': [],\n    'entropies': []\n}\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Starting Training...\")\nprint(\"=\"*60)\nprint(f\"Total timesteps: {total_timesteps:,}\")\nprint(f\"Rollout length: {rollout_length:,}\")\nprint(f\"Observation shape: {state.shape}\")\nprint(f\"Device: {device}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:57:20.527174Z","iopub.execute_input":"2025-12-10T23:57:20.527842Z","iopub.status.idle":"2025-12-10T23:57:20.571367Z","shell.execute_reply.started":"2025-12-10T23:57:20.527811Z","shell.execute_reply":"2025-12-10T23:57:20.570695Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nStarting Training...\n============================================================\nTotal timesteps: 375,000\nRollout length: 4,096\nObservation shape: (4, 84, 84)\nDevice: cuda\n\n","output_type":"stream"}],"execution_count":91},{"id":"31fd3ea1","cell_type":"markdown","source":"## 10. Main Training Loop (Run this cell for training)","metadata":{}},{"id":"ab93c668","cell_type":"code","source":"# Main training loop\nwhile timestep < total_timesteps:\n    # Rollout for rollout_length steps\n    for step in range(rollout_length):\n        # Select action\n        action, log_prob, value = agent.select_action(state, eval_mode=False)\n        \n        # Clip actions to valid range\n        action = np.clip(action, env.action_space.low, env.action_space.high)\n        \n        # Step environment\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        done = terminated or truncated\n        \n        # Convert next_state to uint8 and match CNN input\n        next_state = np.array(next_state, dtype=np.uint8)\n        \n        # Store transition\n        agent.store_transition(state, action, reward, done, log_prob, value)\n        \n        # Update metrics\n        episode_reward += reward\n        episode_length += 1\n        timestep += 1\n        \n        state = next_state\n        \n        # Episode reset\n        if done:\n            episode_rewards.append(episode_reward)\n            episodes_trained += 1\n            \n            print(f\"Episode {episodes_trained}: Reward={episode_reward:.2f}, \"\n                  f\"Length={episode_length}, Timestep={timestep}\")\n            \n            episode_reward = 0\n            episode_length = 0\n            state, _ = env.reset()\n            state = np.array(state, dtype=np.uint8)\n        \n        # Evaluation\n        if timestep % eval_frequency == 0 and timestep > 0:\n            print(f\"\\n--- Evaluation at timestep {timestep:,} ---\")\n            eval_rewards = []\n            \n            for _ in range(eval_episodes):\n                eval_state, _ = eval_env.reset()\n                eval_state = np.array(eval_state, dtype=np.uint8)\n                eval_episode_reward = 0\n                eval_done = False\n                \n                while not eval_done:\n                    eval_action = agent.select_action(eval_state, eval_mode=True)\n                    eval_action = np.clip(eval_action, eval_env.action_space.low, eval_env.action_space.high)\n                    eval_next_state, eval_reward, eval_terminated, eval_truncated, _ = eval_env.step(eval_action)\n                    eval_done = eval_terminated or eval_truncated\n                    eval_episode_reward += eval_reward\n                    eval_state = np.array(eval_next_state, dtype=np.uint8)\n                \n                eval_rewards.append(eval_episode_reward)\n            \n            eval_mean = np.mean(eval_rewards)\n            eval_std = np.std(eval_rewards)\n            print(f\"Eval Mean: {eval_mean:.2f} ± {eval_std:.2f}\")\n            print(f\"Eval Rewards: {eval_rewards}\\n\")\n            \n            training_metrics['timesteps'].append(timestep)\n            training_metrics['eval_rewards'].append(eval_mean)\n            \n            # Save best model\n            if eval_mean > best_eval_reward:\n                best_eval_reward = eval_mean\n                os.makedirs('models', exist_ok=True)\n                model_path = 'models/PPO_CarRacing_best.pth'\n                agent.save(model_path)\n                print(f\"Saved best model to {model_path}\\n\")\n        \n        if timestep >= total_timesteps:\n            break\n    \n    # PPO update\n    if timestep % rollout_length == 0 or timestep >= total_timesteps:\n        print(f\"Timestep {timestep}: PPO Update...\")\n        update_info = agent.update(state)\n        \n        print(f\"  Policy Loss: {update_info['policy_loss']:.4f}\")\n        print(f\"  Value Loss: {update_info['value_loss']:.4f}\")\n        print(f\"  Entropy: {update_info['entropy']:.4f}\\n\")\n        \n        training_metrics['policy_losses'].append(update_info['policy_loss'])\n        training_metrics['value_losses'].append(update_info['value_loss'])\n        training_metrics['entropies'].append(update_info['entropy'])\n        \n        # Periodic save\n        if timestep % save_frequency == 0 and timestep > 0:\n            os.makedirs('models', exist_ok=True)\n            checkpoint_path = f\"models/PPO_CarRacing_checkpoint_{timestep}.pth\"\n            agent.save(checkpoint_path)\n            print(f\"Saved checkpoint to {checkpoint_path}\\n\")\n\n# Final save\nos.makedirs('models', exist_ok=True)\nfinal_model_path = 'models/PPO_CarRacing_final.pth'\nagent.save(final_model_path)\nprint(f\"\\nTraining complete! Final model saved to {final_model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:57:24.454871Z","iopub.execute_input":"2025-12-10T23:57:24.455581Z"}},"outputs":[{"name":"stdout","text":"Episode 1: Reward=-45.21, Length=250, Timestep=250\nEpisode 2: Reward=-51.61, Length=250, Timestep=500\nEpisode 3: Reward=-2.68, Length=250, Timestep=750\nEpisode 4: Reward=-48.72, Length=250, Timestep=1000\nEpisode 5: Reward=-7.53, Length=250, Timestep=1250\nEpisode 6: Reward=-56.67, Length=250, Timestep=1500\nEpisode 7: Reward=-34.58, Length=250, Timestep=1750\nEpisode 8: Reward=-65.03, Length=250, Timestep=2000\nEpisode 9: Reward=-68.31, Length=250, Timestep=2250\nEpisode 10: Reward=-25.17, Length=250, Timestep=2500\nEpisode 11: Reward=-73.24, Length=250, Timestep=2750\nEpisode 12: Reward=-32.10, Length=250, Timestep=3000\nEpisode 13: Reward=-49.79, Length=250, Timestep=3250\nEpisode 14: Reward=-34.55, Length=250, Timestep=3500\nEpisode 15: Reward=-50.76, Length=250, Timestep=3750\nEpisode 16: Reward=-55.39, Length=250, Timestep=4000\nTimestep 4096: PPO Update...\n  Policy Loss: 0.1748\n  Value Loss: 0.5091\n  Entropy: 4.2542\n\nEpisode 17: Reward=-9.42, Length=250, Timestep=4250\nEpisode 18: Reward=-36.80, Length=250, Timestep=4500\nEpisode 19: Reward=-24.91, Length=250, Timestep=4750\nEpisode 20: Reward=24.58, Length=250, Timestep=5000\nEpisode 21: Reward=-41.63, Length=250, Timestep=5250\nEpisode 22: Reward=-36.62, Length=250, Timestep=5500\nEpisode 23: Reward=-77.56, Length=250, Timestep=5750\nEpisode 24: Reward=-51.39, Length=250, Timestep=6000\nEpisode 25: Reward=-64.29, Length=250, Timestep=6250\nEpisode 26: Reward=-82.94, Length=250, Timestep=6500\nEpisode 27: Reward=-40.98, Length=250, Timestep=6750\nEpisode 28: Reward=-37.72, Length=250, Timestep=7000\nEpisode 29: Reward=-27.39, Length=250, Timestep=7250\nEpisode 30: Reward=-65.99, Length=250, Timestep=7500\nEpisode 31: Reward=11.84, Length=250, Timestep=7750\nEpisode 32: Reward=-80.20, Length=250, Timestep=8000\nTimestep 8192: PPO Update...\n  Policy Loss: -0.0096\n  Value Loss: 0.3837\n  Entropy: 4.2150\n\nEpisode 33: Reward=-19.03, Length=250, Timestep=8250\nEpisode 34: Reward=-57.06, Length=250, Timestep=8500\nEpisode 35: Reward=-34.43, Length=250, Timestep=8750\nEpisode 36: Reward=-31.51, Length=250, Timestep=9000\nEpisode 37: Reward=-60.94, Length=250, Timestep=9250\nEpisode 38: Reward=-25.17, Length=250, Timestep=9500\nEpisode 39: Reward=-54.68, Length=250, Timestep=9750\nEpisode 40: Reward=-45.45, Length=250, Timestep=10000\nEpisode 41: Reward=-65.40, Length=250, Timestep=10250\nEpisode 42: Reward=-66.78, Length=250, Timestep=10500\nEpisode 43: Reward=23.81, Length=250, Timestep=10750\nEpisode 44: Reward=8.95, Length=250, Timestep=11000\nEpisode 45: Reward=-71.88, Length=250, Timestep=11250\nEpisode 46: Reward=-40.00, Length=250, Timestep=11500\nEpisode 47: Reward=-37.96, Length=250, Timestep=11750\nEpisode 48: Reward=-32.27, Length=250, Timestep=12000\nEpisode 49: Reward=21.41, Length=250, Timestep=12250\nTimestep 12288: PPO Update...\n  Policy Loss: 0.2386\n  Value Loss: 0.6602\n  Entropy: 4.1806\n\nEpisode 50: Reward=-30.16, Length=250, Timestep=12500\nEpisode 51: Reward=-72.22, Length=250, Timestep=12750\nEpisode 52: Reward=-59.63, Length=250, Timestep=13000\nEpisode 53: Reward=-60.00, Length=250, Timestep=13250\nEpisode 54: Reward=-76.03, Length=250, Timestep=13500\nEpisode 55: Reward=-64.71, Length=250, Timestep=13750\nEpisode 56: Reward=-77.10, Length=250, Timestep=14000\nEpisode 57: Reward=-70.26, Length=250, Timestep=14250\nEpisode 58: Reward=-6.98, Length=250, Timestep=14500\nEpisode 59: Reward=-79.87, Length=250, Timestep=14750\nEpisode 60: Reward=-57.30, Length=250, Timestep=15000\nEpisode 61: Reward=-32.38, Length=250, Timestep=15250\nEpisode 62: Reward=-84.28, Length=250, Timestep=15500\nEpisode 63: Reward=-75.35, Length=250, Timestep=15750\nEpisode 64: Reward=-43.40, Length=250, Timestep=16000\nEpisode 65: Reward=-50.16, Length=250, Timestep=16250\nTimestep 16384: PPO Update...\n  Policy Loss: 0.0518\n  Value Loss: 0.3556\n  Entropy: 4.1584\n\nEpisode 66: Reward=-45.45, Length=250, Timestep=16500\nEpisode 67: Reward=-84.71, Length=250, Timestep=16750\nEpisode 68: Reward=-79.76, Length=250, Timestep=17000\nEpisode 69: Reward=-79.09, Length=250, Timestep=17250\nEpisode 70: Reward=-78.42, Length=250, Timestep=17500\nEpisode 71: Reward=-62.69, Length=250, Timestep=17750\nEpisode 72: Reward=-87.80, Length=250, Timestep=18000\nEpisode 73: Reward=-81.06, Length=250, Timestep=18250\nEpisode 74: Reward=-9.09, Length=250, Timestep=18500\nEpisode 75: Reward=-85.47, Length=250, Timestep=18750\nEpisode 76: Reward=-83.44, Length=250, Timestep=19000\nEpisode 77: Reward=-75.38, Length=250, Timestep=19250\nEpisode 78: Reward=-82.58, Length=250, Timestep=19500\nEpisode 79: Reward=-51.72, Length=250, Timestep=19750\nEpisode 80: Reward=-55.22, Length=250, Timestep=20000\nEpisode 81: Reward=-63.82, Length=250, Timestep=20250\nTimestep 20480: PPO Update...\n  Policy Loss: -0.0018\n  Value Loss: 0.4521\n  Entropy: 4.1135\n\nEpisode 82: Reward=-71.96, Length=250, Timestep=20500\nEpisode 83: Reward=-82.82, Length=250, Timestep=20750\nEpisode 84: Reward=-86.25, Length=250, Timestep=21000\nEpisode 85: Reward=-88.59, Length=250, Timestep=21250\nEpisode 86: Reward=-83.66, Length=250, Timestep=21500\nEpisode 87: Reward=-26.91, Length=250, Timestep=21750\nEpisode 88: Reward=-82.88, Length=250, Timestep=22000\nEpisode 89: Reward=-46.13, Length=250, Timestep=22250\nEpisode 90: Reward=-85.71, Length=250, Timestep=22500\nEpisode 91: Reward=-11.35, Length=250, Timestep=22750\nEpisode 92: Reward=-86.80, Length=250, Timestep=23000\nEpisode 93: Reward=-86.44, Length=250, Timestep=23250\nEpisode 94: Reward=-34.36, Length=250, Timestep=23500\nEpisode 95: Reward=-45.58, Length=250, Timestep=23750\nEpisode 96: Reward=-81.19, Length=250, Timestep=24000\nEpisode 97: Reward=-83.55, Length=250, Timestep=24250\nEpisode 98: Reward=-46.03, Length=250, Timestep=24500\nTimestep 24576: PPO Update...\n  Policy Loss: 0.1277\n  Value Loss: 0.4760\n  Entropy: 4.0973\n\nEpisode 99: Reward=-86.06, Length=250, Timestep=24750\nEpisode 100: Reward=-72.14, Length=250, Timestep=25000\n\n--- Evaluation at timestep 25,000 ---\nEval Mean: -38.33 ± 15.16\nEval Rewards: [-28.315412186380573, -19.00311526479822, -28.571428571429486, -49.82078853046682, -74.99999999999929, -44.82758620689738, -41.97952218430124, -34.64052287581787, -25.000000000000846, -36.10223642172612]\n\nSaved best model to models/PPO_CarRacing_best.pth\n\nEpisode 101: Reward=-88.76, Length=250, Timestep=25250\nEpisode 102: Reward=-41.36, Length=250, Timestep=25500\nEpisode 103: Reward=-29.82, Length=250, Timestep=25750\nEpisode 104: Reward=-89.29, Length=250, Timestep=26000\nEpisode 105: Reward=-78.06, Length=250, Timestep=26250\nEpisode 106: Reward=-55.78, Length=250, Timestep=26500\nEpisode 107: Reward=-66.17, Length=250, Timestep=26750\nEpisode 108: Reward=-80.62, Length=250, Timestep=27000\nEpisode 109: Reward=-82.27, Length=250, Timestep=27250\nEpisode 110: Reward=-90.94, Length=250, Timestep=27500\nEpisode 111: Reward=-90.10, Length=250, Timestep=27750\nEpisode 112: Reward=-20.79, Length=250, Timestep=28000\nEpisode 113: Reward=-48.05, Length=250, Timestep=28250\nEpisode 114: Reward=-61.67, Length=250, Timestep=28500\nTimestep 28672: PPO Update...\n  Policy Loss: 0.0897\n  Value Loss: 0.5405\n  Entropy: 4.0748\n\nEpisode 115: Reward=-66.87, Length=250, Timestep=28750\nEpisode 116: Reward=-61.81, Length=250, Timestep=29000\nEpisode 117: Reward=-46.31, Length=250, Timestep=29250\nEpisode 118: Reward=-88.05, Length=250, Timestep=29500\nEpisode 119: Reward=-72.87, Length=250, Timestep=29750\nEpisode 120: Reward=2.84, Length=250, Timestep=30000\nEpisode 121: Reward=-67.03, Length=250, Timestep=30250\nEpisode 122: Reward=-73.60, Length=250, Timestep=30500\nEpisode 123: Reward=-11.11, Length=250, Timestep=30750\nEpisode 124: Reward=-68.31, Length=250, Timestep=31000\nEpisode 125: Reward=-55.39, Length=250, Timestep=31250\nEpisode 126: Reward=-71.59, Length=250, Timestep=31500\nEpisode 127: Reward=-71.43, Length=250, Timestep=31750\nEpisode 128: Reward=-43.26, Length=250, Timestep=32000\nEpisode 129: Reward=-25.93, Length=250, Timestep=32250\nEpisode 130: Reward=-52.22, Length=250, Timestep=32500\nEpisode 131: Reward=20.62, Length=250, Timestep=32750\nTimestep 32768: PPO Update...\n  Policy Loss: 0.1365\n  Value Loss: 0.4909\n  Entropy: 4.0539\n\nEpisode 132: Reward=-69.18, Length=250, Timestep=33000\nEpisode 133: Reward=-55.41, Length=250, Timestep=33250\nEpisode 134: Reward=-109.36, Length=215, Timestep=33465\nEpisode 135: Reward=-69.28, Length=250, Timestep=33715\nEpisode 136: Reward=-36.17, Length=250, Timestep=33965\nEpisode 137: Reward=-60.45, Length=250, Timestep=34215\nEpisode 138: Reward=-65.78, Length=250, Timestep=34465\nEpisode 139: Reward=-35.48, Length=250, Timestep=34715\nEpisode 140: Reward=-48.39, Length=250, Timestep=34965\nEpisode 141: Reward=-33.57, Length=250, Timestep=35215\nEpisode 142: Reward=-24.91, Length=250, Timestep=35465\nEpisode 143: Reward=-77.85, Length=250, Timestep=35715\nEpisode 144: Reward=-39.80, Length=250, Timestep=35965\nEpisode 145: Reward=-31.03, Length=250, Timestep=36215\nEpisode 146: Reward=-58.99, Length=250, Timestep=36465\nEpisode 147: Reward=-38.08, Length=250, Timestep=36715\nTimestep 36864: PPO Update...\n  Policy Loss: 0.1428\n  Value Loss: 0.4076\n  Entropy: 4.0332\n\nEpisode 148: Reward=-24.73, Length=250, Timestep=36965\nEpisode 149: Reward=-26.83, Length=250, Timestep=37215\nEpisode 150: Reward=-72.32, Length=250, Timestep=37465\nEpisode 151: Reward=-30.00, Length=250, Timestep=37715\nEpisode 152: Reward=-46.11, Length=250, Timestep=37965\nEpisode 153: Reward=-28.08, Length=250, Timestep=38215\nEpisode 154: Reward=66.04, Length=250, Timestep=38465\nEpisode 155: Reward=-3.23, Length=250, Timestep=38715\nEpisode 156: Reward=-57.52, Length=250, Timestep=38965\nEpisode 157: Reward=-2.21, Length=250, Timestep=39215\nEpisode 158: Reward=-20.97, Length=250, Timestep=39465\nEpisode 159: Reward=-74.84, Length=250, Timestep=39715\nEpisode 160: Reward=-21.14, Length=250, Timestep=39965\nEpisode 161: Reward=-25.23, Length=250, Timestep=40215\nEpisode 162: Reward=-65.99, Length=250, Timestep=40465\nEpisode 163: Reward=53.85, Length=250, Timestep=40715\nTimestep 40960: PPO Update...\n  Policy Loss: 0.0388\n  Value Loss: 0.4451\n  Entropy: 4.0060\n\nEpisode 164: Reward=-38.46, Length=250, Timestep=40965\nEpisode 165: Reward=-66.67, Length=250, Timestep=41215\nEpisode 166: Reward=-19.46, Length=250, Timestep=41465\nEpisode 167: Reward=-37.28, Length=250, Timestep=41715\nEpisode 168: Reward=-34.87, Length=250, Timestep=41965\nEpisode 169: Reward=29.28, Length=250, Timestep=42215\nEpisode 170: Reward=-58.96, Length=250, Timestep=42465\nEpisode 171: Reward=-72.13, Length=250, Timestep=42715\nEpisode 172: Reward=-39.39, Length=250, Timestep=42965\nEpisode 173: Reward=-35.36, Length=250, Timestep=43215\nEpisode 174: Reward=-12.59, Length=250, Timestep=43465\nEpisode 175: Reward=-56.04, Length=250, Timestep=43715\nEpisode 176: Reward=-33.82, Length=250, Timestep=43965\nEpisode 177: Reward=-65.52, Length=250, Timestep=44215\nEpisode 178: Reward=-23.84, Length=250, Timestep=44465\nEpisode 179: Reward=-46.37, Length=250, Timestep=44715\nEpisode 180: Reward=-21.43, Length=250, Timestep=44965\nTimestep 45056: PPO Update...\n  Policy Loss: 0.0033\n  Value Loss: 0.3611\n  Entropy: 3.9669\n\nEpisode 181: Reward=-33.33, Length=250, Timestep=45215\nEpisode 182: Reward=-31.97, Length=250, Timestep=45465\nEpisode 183: Reward=-34.26, Length=250, Timestep=45715\nEpisode 184: Reward=-40.79, Length=250, Timestep=45965\nEpisode 185: Reward=-40.77, Length=250, Timestep=46215\nEpisode 186: Reward=29.41, Length=250, Timestep=46465\nEpisode 187: Reward=-13.49, Length=250, Timestep=46715\nEpisode 188: Reward=-74.76, Length=250, Timestep=46965\nEpisode 189: Reward=-60.57, Length=250, Timestep=47215\nEpisode 190: Reward=-65.03, Length=250, Timestep=47465\nEpisode 191: Reward=-84.23, Length=250, Timestep=47715\nEpisode 192: Reward=-40.30, Length=250, Timestep=47965\nEpisode 193: Reward=-73.18, Length=250, Timestep=48215\nEpisode 194: Reward=-62.96, Length=250, Timestep=48465\nEpisode 195: Reward=-42.49, Length=250, Timestep=48715\nEpisode 196: Reward=-23.08, Length=250, Timestep=48965\nTimestep 49152: PPO Update...\n  Policy Loss: 0.1401\n  Value Loss: 0.6015\n  Entropy: 3.9337\n\nEpisode 197: Reward=-51.22, Length=250, Timestep=49215\nEpisode 198: Reward=-58.62, Length=250, Timestep=49465\nEpisode 199: Reward=-4.38, Length=250, Timestep=49715\nEpisode 200: Reward=-65.52, Length=250, Timestep=49965\n\n--- Evaluation at timestep 50,000 ---\nEval Mean: -4.99 ± 23.36\nEval Rewards: [7.9136690647491665, -45.3376205787788, -28.0821917808226, -13.043478260869797, 12.8404669260713, 9.965635738832804, 42.85714285714295, -12.650602409638836, -6.249999999999858, -18.088737201365714]\n\nSaved best model to models/PPO_CarRacing_best.pth\n\nEpisode 201: Reward=-56.23, Length=250, Timestep=50215\nEpisode 202: Reward=-54.20, Length=250, Timestep=50465\nEpisode 203: Reward=-11.11, Length=250, Timestep=50715\nEpisode 204: Reward=-4.76, Length=250, Timestep=50965\nEpisode 205: Reward=-18.83, Length=250, Timestep=51215\nEpisode 206: Reward=-33.33, Length=250, Timestep=51465\nEpisode 207: Reward=12.18, Length=250, Timestep=51715\nEpisode 208: Reward=-27.59, Length=250, Timestep=51965\nEpisode 209: Reward=-43.18, Length=250, Timestep=52215\nEpisode 210: Reward=-40.20, Length=250, Timestep=52465\nEpisode 211: Reward=-4.44, Length=250, Timestep=52715\nEpisode 212: Reward=-10.96, Length=250, Timestep=52965\nEpisode 213: Reward=11.54, Length=250, Timestep=53215\nTimestep 53248: PPO Update...\n  Policy Loss: 0.2467\n  Value Loss: 0.5228\n  Entropy: 3.9170\n\nEpisode 214: Reward=-37.72, Length=250, Timestep=53465\nEpisode 215: Reward=35.85, Length=250, Timestep=53715\nEpisode 216: Reward=-33.75, Length=250, Timestep=53965\nEpisode 217: Reward=5.08, Length=250, Timestep=54215\nEpisode 218: Reward=80.95, Length=250, Timestep=54465\nEpisode 219: Reward=-36.24, Length=250, Timestep=54715\nEpisode 220: Reward=-62.85, Length=250, Timestep=54965\nEpisode 221: Reward=22.26, Length=250, Timestep=55215\nEpisode 222: Reward=50.00, Length=250, Timestep=55465\nEpisode 223: Reward=66.67, Length=250, Timestep=55715\nEpisode 224: Reward=2.11, Length=250, Timestep=55965\nEpisode 225: Reward=-26.20, Length=250, Timestep=56215\nEpisode 226: Reward=-89.98, Length=129, Timestep=56344\nEpisode 227: Reward=4.90, Length=250, Timestep=56594\nEpisode 228: Reward=-31.90, Length=250, Timestep=56844\nEpisode 229: Reward=14.55, Length=250, Timestep=57094\nEpisode 230: Reward=22.88, Length=250, Timestep=57344\nTimestep 57344: PPO Update...\n  Policy Loss: 0.1643\n  Value Loss: 0.3651\n  Entropy: 3.9001\n\nEpisode 231: Reward=-74.46, Length=143, Timestep=57487\nEpisode 232: Reward=26.71, Length=250, Timestep=57737\nEpisode 233: Reward=-21.82, Length=250, Timestep=57987\nEpisode 234: Reward=-27.27, Length=250, Timestep=58237\nEpisode 235: Reward=-3.01, Length=250, Timestep=58487\nEpisode 236: Reward=96.61, Length=250, Timestep=58737\nEpisode 237: Reward=-0.36, Length=250, Timestep=58987\nEpisode 238: Reward=2.41, Length=250, Timestep=59237\nEpisode 239: Reward=-32.93, Length=250, Timestep=59487\nEpisode 240: Reward=30.28, Length=250, Timestep=59737\nEpisode 241: Reward=15.79, Length=250, Timestep=59987\nEpisode 242: Reward=-5.06, Length=250, Timestep=60237\nEpisode 243: Reward=-97.85, Length=159, Timestep=60396\nEpisode 244: Reward=64.23, Length=250, Timestep=60646\nEpisode 245: Reward=13.55, Length=250, Timestep=60896\nEpisode 246: Reward=-4.11, Length=250, Timestep=61146\nEpisode 247: Reward=-17.53, Length=250, Timestep=61396\nTimestep 61440: PPO Update...\n  Policy Loss: 0.0819\n  Value Loss: 0.5056\n  Entropy: 3.8864\n\nEpisode 248: Reward=67.76, Length=250, Timestep=61646\nEpisode 249: Reward=-11.97, Length=250, Timestep=61896\nEpisode 250: Reward=-22.26, Length=250, Timestep=62146\nEpisode 251: Reward=-136.89, Length=195, Timestep=62341\nEpisode 252: Reward=-47.54, Length=250, Timestep=62591\nEpisode 253: Reward=85.19, Length=250, Timestep=62841\nEpisode 254: Reward=28.68, Length=250, Timestep=63091\nEpisode 255: Reward=-121.65, Length=234, Timestep=63325\nEpisode 256: Reward=11.86, Length=250, Timestep=63575\nEpisode 257: Reward=28.21, Length=250, Timestep=63825\nEpisode 258: Reward=52.78, Length=250, Timestep=64075\nEpisode 259: Reward=141.88, Length=250, Timestep=64325\nEpisode 260: Reward=-6.61, Length=250, Timestep=64575\nEpisode 261: Reward=40.35, Length=250, Timestep=64825\nEpisode 262: Reward=7.69, Length=250, Timestep=65075\nEpisode 263: Reward=-4.92, Length=250, Timestep=65325\nTimestep 65536: PPO Update...\n  Policy Loss: 0.0626\n  Value Loss: 0.4464\n  Entropy: 3.8675\n\nEpisode 264: Reward=18.42, Length=250, Timestep=65575\nEpisode 265: Reward=70.07, Length=250, Timestep=65825\nEpisode 266: Reward=-10.45, Length=250, Timestep=66075\nEpisode 267: Reward=-28.32, Length=250, Timestep=66325\nEpisode 268: Reward=40.00, Length=250, Timestep=66575\nEpisode 269: Reward=-44.08, Length=250, Timestep=66825\nEpisode 270: Reward=1.89, Length=250, Timestep=67075\nEpisode 271: Reward=13.88, Length=250, Timestep=67325\nEpisode 272: Reward=-24.46, Length=250, Timestep=67575\nEpisode 273: Reward=-33.99, Length=250, Timestep=67825\nEpisode 274: Reward=-65.88, Length=108, Timestep=67933\nEpisode 275: Reward=-17.27, Length=250, Timestep=68183\nEpisode 276: Reward=-12.59, Length=250, Timestep=68433\nEpisode 277: Reward=-88.24, Length=131, Timestep=68564\nEpisode 278: Reward=-34.26, Length=250, Timestep=68814\nEpisode 279: Reward=-15.86, Length=250, Timestep=69064\nEpisode 280: Reward=-131.98, Length=249, Timestep=69313\nEpisode 281: Reward=-2.60, Length=250, Timestep=69563\nTimestep 69632: PPO Update...\n  Policy Loss: 0.0983\n  Value Loss: 0.5074\n  Entropy: 3.8411\n\nEpisode 282: Reward=-3.23, Length=250, Timestep=69813\nEpisode 283: Reward=-48.10, Length=250, Timestep=70063\nEpisode 284: Reward=11.11, Length=250, Timestep=70313\nEpisode 285: Reward=-28.08, Length=250, Timestep=70563\nEpisode 286: Reward=106.29, Length=250, Timestep=70813\nEpisode 287: Reward=-12.70, Length=250, Timestep=71063\nEpisode 288: Reward=-75.18, Length=250, Timestep=71313\nEpisode 289: Reward=-23.59, Length=250, Timestep=71563\nEpisode 290: Reward=38.46, Length=250, Timestep=71813\nEpisode 291: Reward=-1.69, Length=250, Timestep=72063\nEpisode 292: Reward=-65.41, Length=250, Timestep=72313\nEpisode 293: Reward=-54.55, Length=250, Timestep=72563\nEpisode 294: Reward=-44.44, Length=250, Timestep=72813\nEpisode 295: Reward=-30.77, Length=250, Timestep=73063\nEpisode 296: Reward=-38.91, Length=250, Timestep=73313\nEpisode 297: Reward=-4.93, Length=250, Timestep=73563\nTimestep 73728: PPO Update...\n  Policy Loss: 0.0643\n  Value Loss: 0.4917\n  Entropy: 3.8204\n\nEpisode 298: Reward=70.82, Length=250, Timestep=73813\nEpisode 299: Reward=-14.37, Length=250, Timestep=74063\nEpisode 300: Reward=-6.98, Length=250, Timestep=74313\nEpisode 301: Reward=7.27, Length=250, Timestep=74563\nEpisode 302: Reward=-41.54, Length=250, Timestep=74813\n\n--- Evaluation at timestep 75,000 ---\nEval Mean: 61.84 ± 122.50\nEval Rewards: [210.8974358974398, -54.26829268292759, 57.89473684210891, -52.38095238095312, -31.137724550898927, -2.43902439024453, 0.6944444444444815, -17.525773195876926, 202.5477707006412, 304.10958904109725]\n\nSaved best model to models/PPO_CarRacing_best.pth\n\nEpisode 303: Reward=-45.69, Length=250, Timestep=75063\nEpisode 304: Reward=-99.52, Length=140, Timestep=75203\nEpisode 305: Reward=-3.33, Length=250, Timestep=75453\nEpisode 306: Reward=50.00, Length=250, Timestep=75703\nEpisode 307: Reward=74.50, Length=250, Timestep=75953\nEpisode 308: Reward=-33.80, Length=250, Timestep=76203\nEpisode 309: Reward=45.83, Length=250, Timestep=76453\nEpisode 310: Reward=-51.49, Length=250, Timestep=76703\nEpisode 311: Reward=-68.25, Length=250, Timestep=76953\nEpisode 312: Reward=-45.21, Length=250, Timestep=77203\nEpisode 313: Reward=10.29, Length=250, Timestep=77453\nEpisode 314: Reward=-36.80, Length=250, Timestep=77703\nTimestep 77824: PPO Update...\n  Policy Loss: -0.0277\n  Value Loss: 0.2714\n  Entropy: 3.7707\n\nEpisode 315: Reward=-27.71, Length=250, Timestep=77953\nEpisode 316: Reward=30.43, Length=250, Timestep=78203\nEpisode 317: Reward=16.50, Length=250, Timestep=78453\nEpisode 318: Reward=-69.70, Length=250, Timestep=78703\nEpisode 319: Reward=-83.22, Length=250, Timestep=78953\nEpisode 320: Reward=-78.18, Length=250, Timestep=79203\nEpisode 321: Reward=39.62, Length=250, Timestep=79453\nEpisode 322: Reward=-58.21, Length=250, Timestep=79703\nEpisode 323: Reward=-46.38, Length=250, Timestep=79953\nEpisode 324: Reward=-37.89, Length=250, Timestep=80203\nEpisode 325: Reward=39.93, Length=250, Timestep=80453\nEpisode 326: Reward=3.20, Length=250, Timestep=80703\nEpisode 327: Reward=-5.54, Length=250, Timestep=80953\nEpisode 328: Reward=-33.33, Length=250, Timestep=81203\nEpisode 329: Reward=39.19, Length=250, Timestep=81453\nEpisode 330: Reward=27.95, Length=250, Timestep=81703\nTimestep 81920: PPO Update...\n  Policy Loss: 0.3421\n  Value Loss: 0.4026\n  Entropy: 3.7304\n\nEpisode 331: Reward=-59.32, Length=250, Timestep=81953\nEpisode 332: Reward=-9.94, Length=250, Timestep=82203\nEpisode 333: Reward=-9.09, Length=250, Timestep=82453\nEpisode 334: Reward=-1.32, Length=250, Timestep=82703\nEpisode 335: Reward=-11.39, Length=250, Timestep=82953\nEpisode 336: Reward=-54.98, Length=250, Timestep=83203\nEpisode 337: Reward=15.08, Length=250, Timestep=83453\nEpisode 338: Reward=68.79, Length=250, Timestep=83703\nEpisode 339: Reward=15.65, Length=250, Timestep=83953\nEpisode 340: Reward=-7.89, Length=250, Timestep=84203\nEpisode 341: Reward=-49.46, Length=250, Timestep=84453\nEpisode 342: Reward=-37.50, Length=250, Timestep=84703\nEpisode 343: Reward=-23.64, Length=250, Timestep=84953\nEpisode 344: Reward=69.44, Length=250, Timestep=85203\nEpisode 345: Reward=-21.38, Length=250, Timestep=85453\nEpisode 346: Reward=62.25, Length=250, Timestep=85703\nEpisode 347: Reward=-66.67, Length=250, Timestep=85953\nTimestep 86016: PPO Update...\n  Policy Loss: 0.0121\n  Value Loss: 0.3562\n  Entropy: 3.6946\n\nEpisode 348: Reward=25.90, Length=250, Timestep=86203\nEpisode 349: Reward=27.45, Length=250, Timestep=86453\nEpisode 350: Reward=-41.78, Length=250, Timestep=86703\nEpisode 351: Reward=-63.46, Length=250, Timestep=86953\nEpisode 352: Reward=-62.33, Length=250, Timestep=87203\nEpisode 353: Reward=-6.50, Length=250, Timestep=87453\nEpisode 354: Reward=-44.88, Length=250, Timestep=87703\nEpisode 355: Reward=128.92, Length=250, Timestep=87953\nEpisode 356: Reward=12.73, Length=250, Timestep=88203\nEpisode 357: Reward=-39.39, Length=250, Timestep=88453\nEpisode 358: Reward=-47.76, Length=250, Timestep=88703\nEpisode 359: Reward=3.23, Length=250, Timestep=88953\nEpisode 360: Reward=-52.52, Length=250, Timestep=89203\nEpisode 361: Reward=-38.70, Length=250, Timestep=89453\nEpisode 362: Reward=15.65, Length=250, Timestep=89703\nEpisode 363: Reward=-17.86, Length=250, Timestep=89953\nTimestep 90112: PPO Update...\n  Policy Loss: 0.0279\n  Value Loss: 0.3845\n  Entropy: 3.6559\n\nEpisode 364: Reward=-64.86, Length=250, Timestep=90203\nEpisode 365: Reward=-33.88, Length=250, Timestep=90453\nEpisode 366: Reward=101.37, Length=250, Timestep=90703\nEpisode 367: Reward=-29.29, Length=250, Timestep=90953\nEpisode 368: Reward=-44.03, Length=209, Timestep=91162\nEpisode 369: Reward=111.47, Length=250, Timestep=91412\nEpisode 370: Reward=140.16, Length=250, Timestep=91662\nEpisode 371: Reward=-50.66, Length=250, Timestep=91912\nEpisode 372: Reward=27.27, Length=250, Timestep=92162\nEpisode 373: Reward=-8.45, Length=250, Timestep=92412\nEpisode 374: Reward=70.82, Length=250, Timestep=92662\nEpisode 375: Reward=33.59, Length=250, Timestep=92912\nEpisode 376: Reward=47.16, Length=250, Timestep=93162\nEpisode 377: Reward=10.00, Length=250, Timestep=93412\nEpisode 378: Reward=-9.09, Length=250, Timestep=93662\nEpisode 379: Reward=-11.35, Length=250, Timestep=93912\nEpisode 380: Reward=-87.80, Length=88, Timestep=94000\nTimestep 94208: PPO Update...\n  Policy Loss: 0.0567\n  Value Loss: 0.4704\n  Entropy: 3.6244\n\nEpisode 381: Reward=14.50, Length=250, Timestep=94250\nEpisode 382: Reward=-25.00, Length=250, Timestep=94500\nEpisode 383: Reward=-29.89, Length=250, Timestep=94750\nEpisode 384: Reward=96.91, Length=250, Timestep=95000\nEpisode 385: Reward=-30.50, Length=250, Timestep=95250\nEpisode 386: Reward=25.93, Length=250, Timestep=95500\nEpisode 387: Reward=12.96, Length=250, Timestep=95750\nEpisode 388: Reward=-78.34, Length=250, Timestep=96000\nEpisode 389: Reward=-33.56, Length=194, Timestep=96194\nEpisode 390: Reward=4.87, Length=250, Timestep=96444\nEpisode 391: Reward=-59.18, Length=250, Timestep=96694\nEpisode 392: Reward=-24.53, Length=250, Timestep=96944\nEpisode 393: Reward=-71.43, Length=250, Timestep=97194\nEpisode 394: Reward=-50.50, Length=250, Timestep=97444\nEpisode 395: Reward=-31.15, Length=250, Timestep=97694\nEpisode 396: Reward=-2.78, Length=250, Timestep=97944\nEpisode 397: Reward=-0.87, Length=250, Timestep=98194\nTimestep 98304: PPO Update...\n  Policy Loss: 0.0358\n  Value Loss: 0.3431\n  Entropy: 3.5885\n\nEpisode 398: Reward=7.94, Length=250, Timestep=98444\nEpisode 399: Reward=-108.09, Length=182, Timestep=98626\nEpisode 400: Reward=39.29, Length=250, Timestep=98876\nEpisode 401: Reward=-86.78, Length=124, Timestep=99000\nEpisode 402: Reward=-62.96, Length=250, Timestep=99250\nEpisode 403: Reward=-16.40, Length=250, Timestep=99500\nEpisode 404: Reward=-53.85, Length=250, Timestep=99750\nEpisode 405: Reward=-42.86, Length=250, Timestep=100000\n\n--- Evaluation at timestep 100,000 ---\nEval Mean: 362.63 ± 219.28\nEval Rewards: [295.2569169960462, 30.732616487458102, 383.5526315789353, 256.16438356164156, 561.9217081850419, 270.1298701298617, 508.39160839159814, 697.7528089887529, 22.03389830508651, 600.3610108303125]\n\nSaved best model to models/PPO_CarRacing_best.pth\n\nEpisode 406: Reward=-14.09, Length=250, Timestep=100250\nEpisode 407: Reward=36.67, Length=250, Timestep=100500\nEpisode 408: Reward=59.53, Length=250, Timestep=100750\n","output_type":"stream"}],"execution_count":null},{"id":"11ea6f32","cell_type":"markdown","source":"## 11. Save Training Statistics","metadata":{}},{"id":"ad60ceb4","cell_type":"code","source":"# Save episode rewards\nos.makedirs('results', exist_ok=True)\nresults_file = 'results/PPO_CarRacing_training_rewards.csv'\n\nwith open(results_file, 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['Episode', 'Reward'])\n    for i, reward in enumerate(episode_rewards):\n        writer.writerow([i + 1, reward])\n\nprint(f\"Training rewards saved to {results_file}\")\nprint(f\"Total episodes trained: {episodes_trained}\")\nprint(f\"Total timesteps: {timestep:,}\")\nprint(f\"\\nTraining Statistics:\")\nif episode_rewards:\n    print(f\"Mean episode reward: {np.mean(episode_rewards):.2f}\")\n    print(f\"Std episode reward: {np.std(episode_rewards):.2f}\")\n    print(f\"Max episode reward: {np.max(episode_rewards):.2f}\")\n    print(f\"Min episode reward: {np.min(episode_rewards):.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:55:22.914613Z","iopub.status.idle":"2025-12-10T23:55:22.914810Z","shell.execute_reply.started":"2025-12-10T23:55:22.914711Z","shell.execute_reply":"2025-12-10T23:55:22.914720Z"}},"outputs":[],"execution_count":null},{"id":"3a103844","cell_type":"markdown","source":"## 12. Visualization of Training Progress","metadata":{}},{"id":"c874b576","cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Episode rewards over time\naxes[0, 0].plot(episode_rewards, label='Episode Reward')\naxes[0, 0].set_xlabel('Episode')\naxes[0, 0].set_ylabel('Reward')\naxes[0, 0].set_title('Episode Rewards During Training')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].legend()\n\n# Running average of episode rewards\nif episode_rewards:\n    window = min(50, len(episode_rewards))\n    running_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n    axes[0, 1].plot(running_avg, label=f'Running Average (window={window})', color='orange')\n    axes[0, 1].set_xlabel('Episode')\n    axes[0, 1].set_ylabel('Reward')\n    axes[0, 1].set_title('Running Average of Episode Rewards')\n    axes[0, 1].grid(True, alpha=0.3)\n    axes[0, 1].legend()\n\n# Policy and Value losses\nif training_metrics['policy_losses']:\n    axes[1, 0].plot(training_metrics['policy_losses'], label='Policy Loss', marker='o')\n    axes[1, 0].plot(training_metrics['value_losses'], label='Value Loss', marker='s')\n    axes[1, 0].set_xlabel('Update')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].set_title('Policy and Value Losses')\n    axes[1, 0].grid(True, alpha=0.3)\n    axes[1, 0].legend()\n    axes[1, 0].set_yscale('log')\n\n# Evaluation rewards\nif training_metrics['eval_rewards']:\n    axes[1, 1].plot(training_metrics['timesteps'], training_metrics['eval_rewards'], \n                     label='Eval Mean Reward', marker='o', color='green', linewidth=2)\n    axes[1, 1].set_xlabel('Timestep')\n    axes[1, 1].set_ylabel('Reward')\n    axes[1, 1].set_title('Evaluation Mean Rewards')\n    axes[1, 1].grid(True, alpha=0.3)\n    axes[1, 1].legend()\n\nplt.tight_layout()\nplt.savefig('results/training_progress.png', dpi=150)\nplt.show()\n\nprint(\"Training progress visualization saved to results/training_progress.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:55:22.915995Z","iopub.status.idle":"2025-12-10T23:55:22.916347Z","shell.execute_reply.started":"2025-12-10T23:55:22.916165Z","shell.execute_reply":"2025-12-10T23:55:22.916180Z"}},"outputs":[],"execution_count":null},{"id":"3d33b2ba","cell_type":"markdown","source":"## 13. Test Trained Agent","metadata":{}},{"id":"e059edf8","cell_type":"code","source":"# Load the best trained model\ntest_env = gym.make('CarRacing-v3', continuous=True, render_mode=None)\ntest_env = CarRacingPreprocessor.apply(test_env, use_grayscale=True, num_frames=4)\n\n# Create agent for testing\ntest_agent = PPOAgentCNN(\n    action_dim=test_env.action_space.shape[0],\n    hyperparameters=HYPERPARAMETERS,\n    device=device,\n    feature_dim=256,\n    input_channels=4\n)\n\n# Load best model\nmodel_path = 'models/PPO_CarRacing_best.pth'\nif os.path.exists(model_path):\n    test_agent.load(model_path)\n    print(f\"Loaded model from {model_path}\")\nelse:\n    print(f\"Warning: Model file not found at {model_path}\")\n    print(\"Using current agent for testing\")\n\n# Test for 5 episodes\ntest_episodes = 100\ntest_rewards = []\n\nprint(f\"\\nTesting trained agent for {test_episodes} episodes...\\n\")\n\nfor ep in range(test_episodes):\n    test_state, _ = test_env.reset()\n    test_reward = 0\n    test_done = False\n    \n    while not test_done:\n        test_action = test_agent.select_action(test_state, eval_mode=True)\n        test_action = np.clip(test_action, test_env.action_space.low, test_env.action_space.high)\n        test_next_state, test_r, test_terminated, test_truncated, _ = test_env.step(test_action)\n        test_done = test_terminated or test_truncated\n        test_reward += test_r\n        test_state = test_next_state\n    \n    test_rewards.append(test_reward)\n    print(f\"Test Episode {ep+1}: Reward = {test_reward:.2f}\")\n\ntest_env.close()\n\nprint(f\"\\nTest Statistics:\")\nprint(f\"Mean test reward: {np.mean(test_rewards):.2f}\")\nprint(f\"Std test reward: {np.std(test_rewards):.2f}\")\nprint(f\"Max test reward: {np.max(test_rewards):.2f}\")\nprint(f\"Min test reward: {np.min(test_rewards):.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:55:22.917690Z","iopub.status.idle":"2025-12-10T23:55:22.918014Z","shell.execute_reply.started":"2025-12-10T23:55:22.917842Z","shell.execute_reply":"2025-12-10T23:55:22.917857Z"}},"outputs":[],"execution_count":null},{"id":"f5aca643","cell_type":"markdown","source":"## 14. Summary and Notes\n\n### Training Complete!\n\n**Key Achievements:**\n- Implemented PPO with CNN encoder for visual input processing\n- Applied grayscale conversion and frame stacking for efficient observation handling\n- Successfully trained agent on CarRacing-v3 environment\n- Saved checkpoints and best models\n- Evaluated agent performance\n\n**Model Architecture:**\n- **CNN Encoder**: 4 input channels (stacked grayscale frames) → 256-dim feature vector\n- **Actor Network**: 256-dim input → 2 hidden layers (256) → continuous actions\n- **Critic Network**: 256-dim input → 2 hidden layers (256) → scalar value\n\n**Next Steps for Kaggle Training:**\n1. Adjust `total_timesteps` for longer training (2-3M steps recommended)\n2. Fine-tune learning rate and entropy coefficient\n3. Consider curriculum learning or reward shaping\n4. Monitor GPU memory usage\n5. Save regular checkpoints for recovery\n\n**Files Generated:**\n- `models/PPO_CarRacing_best.pth` - Best model by evaluation reward\n- `models/PPO_CarRacing_final.pth` - Final model after training\n- `results/PPO_CarRacing_training_rewards.csv` - Episode rewards log\n- `results/training_progress.png` - Training visualization","metadata":{}}]}